{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amal-Baby-Mathews/GeminiAPIComp/blob/main/Genvision_Beta_testing_flask_included.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwqForAWlOF_",
        "outputId": "296dfcde-ca19-49ae-8a11-841427adc668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (2.1.5)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask\n",
        "!pip install pyngrok\n",
        "!ngrok authtoken cr_2g8gI0341YlFhuK5d3aWV6mBHzc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlPVRVqbrwvA",
        "outputId": "02360f48-f423-4d0e-d1d2-487815856f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting langchain-core<0.3,>=0.2.27 (from langgraph)\n",
            "  Downloading langchain_core-0.2.29-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-1.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2.27->langgraph)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2.27->langgraph)\n",
            "  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (2.8.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3,>=0.2.27->langgraph)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.27->langgraph) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.2.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.27->langgraph)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.27->langgraph)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m861.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.27->langgraph) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.27->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.27->langgraph) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.27->langgraph) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.27->langgraph) (2.0.7)\n",
            "Downloading langgraph-0.2.3-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.29-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-1.0.2-py3-none-any.whl (14 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.98-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, langsmith, httpx, langchain-core, langgraph-checkpoint, langgraph\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.29 langgraph-0.2.3 langgraph-checkpoint-1.0.2 langsmith-0.1.98 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph httpx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z-c_76eCrwvE"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAgE2iXkrwvH",
        "outputId": "0188a8da-7ff8-4c57-ba5a-67ca3f722da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.29)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.98)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.2.12-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.2.12 langchain-text-splitters-0.2.2 langchain_community-0.2.11 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkvHX5E3rwvJ"
      },
      "source": [
        "# **Code for Genvis! Let's do this!**\n",
        "Using langraph, created a langraph app to do the following:\n",
        "Greet user\n",
        "prompt user to take photo\n",
        "get photo description\n",
        "describe the photo to user ask for feedback(feedback adds to the prompt on the changes to the description of the photo)\n",
        "using the feedback to make new description of the photo\n",
        "ask user for more clarification if needed or to take new photo for analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xzpIIcQhqSdI"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "#img = PIL.Image.open('path/to/image.png')\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "#response = model.generate_content([\"What is in this photo?\", img])\n",
        "#print(response.text)\n",
        "import requests\n",
        "from typing import Dict, Any\n",
        "\n",
        "def google_search(api_key: str, cx: str, query: str) -> Dict[str, Any]:\n",
        "    base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    params = {\n",
        "        'key': api_key,\n",
        "        'cx': cx,\n",
        "        'q': query\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        response.raise_for_status()\n",
        "\n",
        "api_key=userdata.get('GEMINI_API_KEY')\n",
        "cx = '16f55fa2cee4d4e0b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAUCT2Oxg4uD"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, TypedDict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from PIL import Image\n",
        "class GraphState(TypedDict):\n",
        "    user_input: Optional[list[str]]\n",
        "    photo_description: Optional[str]\n",
        "    feedback: Optional[str]\n",
        "    llm_output: Optional[str]\n",
        "    image_path: Optional[str]\n",
        "def first_user_input(state: GraphState) -> GraphState:\n",
        "    return state\n",
        "from typing import Literal\n",
        "\n",
        "def decision(state: GraphState) -> Literal[\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\", 'Search the web for particular detail']:\n",
        "      prompt = f\"\"\"You are a decision making intelligent machine inside the pipeline of an app which helps the visually impaired. The user is querying about the description of the photo or Chatting. Your task is to decide what the next step is by selecting what the output string is according to the user input. Output 'Fetch details from the image' if no user_input is found.Output the single string without quotes. Only output the exact corresponding string chosen from the following strings:\n",
        "                  'Take a photo', 'Fetch details from the image', 'Save the photo', 'Take a video', 'Search the web for particular detail', 'Use Chat bot' and 'Error'. This is the user's input: {state[\"user_input\"][-1]}\"\"\"\n",
        "      response = model.generate_content(prompt)\n",
        "      state[\"llm_output\"] = response.text.strip()\n",
        "\n",
        "      valid_outputs = {\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\",'Search the web for particular detail'}\n",
        "\n",
        "      if state[\"llm_output\"] in valid_outputs:\n",
        "          return state[\"llm_output\"]  # Return the string as a Literal\n",
        "      else:\n",
        "          state[\"llm_output\"]=\"Error\"\n",
        "          return state[\"llm_output\"]\n",
        "\n",
        "def take_photo(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to take a photo and update the state\n",
        "    state[\"llm_output\"] = \"Photo taken\"\n",
        "    return state\n",
        "def fetch_details(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to fetch details from the photo and update the state\n",
        "    if state[\"image_path\"] is not None:\n",
        "      img = Image.open(state[\"image_path\"])\n",
        "      state[\"photo_description\"]=model.generate_content([f\"You are an assistant for a blind user. You are very kind and accommodating.Refer to the image as view.The user's input is {state['user_input'][-1]}.Describe the view according to the user's request: \" , img]).text\n",
        "      state[\"llm_output\"] = state[\"photo_description\"]\n",
        "    else:\n",
        "      state[\"llm_output\"] = \"Could not find the image\"\n",
        "    return state\n",
        "def chat_bot(state: GraphState) -> GraphState:\n",
        "    response=model.generate_content([f\"You are an assistant for a blind user. You are very kind. Answer the User's query: {state['user_input'][-1]}\"])\n",
        "    state[\"llm_output\"]=response.text\n",
        "    return state\n",
        "def web_search(state:GraphState) -> GraphState:\n",
        "    search_list=[f\"Generate a short single line Web search query to answer user's query: {state['user_input'][-1]}. Use the image to understand the query if necessary\"]\n",
        "    if state[\"image_path\"] is not None:\n",
        "      img = Image.open(state[\"image_path\"])\n",
        "      search_list.append(img)\n",
        "    #Logic for searching the internet\n",
        "    search_prompt=model.generate_content(search_list)\n",
        "    search_prompt=search_prompt.text.strip('\"')\n",
        "    #Implement Search using Google Custom API:\n",
        "    search_results = google_search(api_key, cx, search_prompt)\n",
        "    print(search_results)\n",
        "    result=\"\"\n",
        "    for item in search_results.get('items', [])[:3]:\n",
        "        result+=f\"Title: {item['title']}\\nSnippet: {item['snippet']}\\n\\n\"\n",
        "    if result==\"\":\n",
        "      result=\"No results found\"\n",
        "    state[\"feedback\"]=result\n",
        "    result=model.generate_content([f\"You are an assistant for a blind user. You are very straightforward.Refer to the search result:{result}, and try to answer user's query: {state['user_input'][-1]}. \"])\n",
        "    state[\"llm_output\"]=result.text\n",
        "    return state\n",
        "def save_photo(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to save the photo and update the state\n",
        "    state[\"llm_output\"] = \"Photo saved\"\n",
        "    return state\n",
        "def take_video(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to take a video and update the state\n",
        "    state[\"llm_output\"] = \"Video taken\"\n",
        "    return state\n",
        "def error(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to handle errors and update the state\n",
        "    state[\"llm_output\"]= \"Some internal error occured Please retry\"\n",
        "    return state\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"first_user_input\", first_user_input)\n",
        "\n",
        "workflow.add_node(\"take_photo\", take_photo)\n",
        "workflow.add_node(\"fetch_details\", fetch_details)\n",
        "workflow.add_node(\"save_photo\", save_photo)\n",
        "workflow.add_node(\"take_video\", take_video)\n",
        "workflow.add_node(\"error\", error)\n",
        "workflow.add_node(\"web_search\", web_search)\n",
        "workflow.add_node(\"chat_bot\", chat_bot)\n",
        "workflow.set_entry_point(\"first_user_input\")\n",
        "workflow.add_edge(\"take_photo\", END)\n",
        "workflow.add_edge(\"fetch_details\", END)\n",
        "workflow.add_edge(\"save_photo\", END)\n",
        "workflow.add_edge(\"take_video\", END)\n",
        "workflow.add_conditional_edges(\n",
        "    \"first_user_input\",\n",
        "    decision,\n",
        "    {\n",
        "        \"Take a photo\": \"take_photo\",\n",
        "        \"Fetch details from the image\": \"fetch_details\",\n",
        "        \"Save the photo\": \"save_photo\",\n",
        "        \"Take a video\": \"take_video\",\n",
        "        \"Error\": \"error\",\n",
        "        \"Use Chat bot\": \"chat_bot\",\n",
        "        \"Search the web for particular detail\": \"web_search\",\n",
        "    },\n",
        ")\n",
        "\n",
        "app = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Be3Zet5Yl0VK",
        "outputId": "0fa37127-2895-4c2b-9b2a-51167949957f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'user_input': ['What is the age of enthiran', 'Hi', 'what is in this image?'],\n",
              " 'photo_description': 'Sure, I can help you with that.  The view is a park with a brick pathway that curves through the green grass. There is a wooden bench on the left side of the path.  There are trees all around the path and a gazebo in the background. It looks like a lovely place to relax and enjoy the fresh air. \\n',\n",
              " 'feedback': None,\n",
              " 'llm_output': 'Sure, I can help you with that.  The view is a park with a brick pathway that curves through the green grass. There is a wooden bench on the left side of the path.  There are trees all around the path and a gazebo in the background. It looks like a lovely place to relax and enjoy the fresh air. \\n',\n",
              " 'image_path': '/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg'}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke({\"user_input\":[\"What is the age of enthiran\",\"Hi\",\"what is in this image?\"],\"photo_description\":None, \"feedback\": None, \"llm_output\":None, \"image_path\":\"/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Xz330lCLqKHD"
      },
      "outputs": [],
      "source": [
        "Base_prompt=\"\"\"You are a helpful and attentive AI assistant designed to describe photos for blind users. Your primary goal is to provide vivid, detailed descriptions that help users \"see\" through your words. Focus on elements that convey the overall atmosphere, mood, and context of the image. Describe colors, textures, spatial relationships, and any interesting or unusual details. Be specific about the positioning of objects and people. Mention facial expressions, body language, and clothing when relevant. For outdoor scenes, describe the weather, time of day, and natural elements. For indoor photos, explain the type of room and its furnishings. Always ask if the user would like more details about any particular aspect of the image. Be patient and willing to repeat or clarify information. Offer to break down complex images into smaller parts for easier understanding. Use descriptive language that appeals to other senses, like describing how things might feel or smell. Your tone should be warm, engaging, and encouraging, making the experience of \"viewing\" photos enjoyable and informative for the user.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ycnPmqM5eN10"
      },
      "outputs": [],
      "source": [
        "#Main code for flask api\n",
        "from typing import Optional, TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, END\n",
        "from PIL import Image\n",
        "class GraphState(TypedDict):\n",
        "    user_input: Optional[list[str]]\n",
        "    photo_description: Optional[str]\n",
        "    feedback: Optional[str]\n",
        "    llm_output: Optional[str]\n",
        "    image_path: Optional[str]\n",
        "    chat_history: Optional[list[str]]\n",
        "\n",
        "def get_last_two_chats(chat_history: list[str]) -> str:\n",
        "    his=\" \".join(chat_history[-4:]) if len(chat_history) >= 4 else \" \".join(chat_history)\n",
        "    return his\n",
        "\n",
        "def first_user_input(state: GraphState) -> GraphState:\n",
        "    if state.get(\"chat_history\") is None:\n",
        "        state[\"chat_history\"] = []\n",
        "    return state\n",
        "\n",
        "def decision(state: GraphState) -> Literal[\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\", 'Search the web for particular detail']:\n",
        "    last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "    prompt = f\"\"\"You are a decision making intelligent machine inside the pipeline of an app which helps the visually impaired. Use the internet for extra information. Only take a photo if the user clearly asks for the same.The user is querying about the description of the photo or Chatting. Your task is to decide what the next step is by selecting what the output string is according to the user input. Output 'Fetch details from the image' if no user_input is found. Redirect all innappropriate comments to the chatbot. Output the single string without quotes. Only output the exact corresponding string chosen from the following strings:\n",
        "                'Take a photo', 'Fetch details from the image', 'Save the photo', 'Take a video', 'Search the web for particular detail', 'Use Chat bot' and 'Error'.\n",
        "                This is the user's input: {state[\"user_input\"][-1]}\n",
        "                Recent chat history: {last_two_chats}\"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    state[\"llm_output\"] = response.text.strip()\n",
        "\n",
        "    valid_outputs = {\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\", 'Search the web for particular detail'}\n",
        "\n",
        "    if state[\"llm_output\"] in valid_outputs:\n",
        "        return state[\"llm_output\"]\n",
        "    else:\n",
        "        state[\"llm_output\"] = \"Error\"\n",
        "        return state[\"llm_output\"]\n",
        "\n",
        "def take_photo(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Photo taken\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def fetch_details(state: GraphState) -> GraphState:\n",
        "    if state[\"image_path\"] is not None:\n",
        "        img = Image.open(state[\"image_path\"])\n",
        "        last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "        state[\"photo_description\"] = model.generate_content([\n",
        "            f\"{Base_prompt} Refer to the image as view. The user's input is {state['user_input'][-1]}. Recent chat history: {last_two_chats}. Describe the view according to the user's request: \",\n",
        "            img\n",
        "        ]).text\n",
        "        state[\"llm_output\"] = state[\"photo_description\"]\n",
        "    else:\n",
        "        state[\"llm_output\"] = \"Could not find the image\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def chat_bot(state: GraphState) -> GraphState:\n",
        "    last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "    prompt=last_two_chats\n",
        "    response = model.generate_content([f\"You are an assistant for a blind user. You are extremely kind. You are unbelievably helpful and the Epitome of Innovation.Your tasks are strictly limited to taking a photo, answering users questions, take a video, saving taken video or photo, search the internet or be friends with the user. Recent chat history: {last_two_chats}. Answer the User's query: {state['user_input'][-1]}\"])\n",
        "    state[\"llm_output\"] = response.text\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def web_search(state: GraphState) -> GraphState:\n",
        "    last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "    search_list = [f\"Generate a short single line Web search query to answer user's query: {state['user_input'][-1]}. Recent chat history: {last_two_chats}. Use the image to understand the query if necessary\"]\n",
        "    if state[\"image_path\"] is not None:\n",
        "        img = Image.open(state[\"image_path\"])\n",
        "        search_list.append(img)\n",
        "    search_prompt = model.generate_content(search_list)\n",
        "    search_prompt = search_prompt.text.strip('\"')\n",
        "    search_results = google_search(api_key, cx, search_prompt)\n",
        "    result = \"\"\n",
        "    for item in search_results.get('items', [])[:3]:\n",
        "        result += f\"Title: {item['title']}\\nSnippet: {item['snippet']}\\n\\n\"\n",
        "    if result == \"\":\n",
        "        result = \"No results found\"\n",
        "    state[\"feedback\"] = result\n",
        "    result = model.generate_content([f\"You are an assistant for a blind user. You are very straightforward. Refer to the search result:{result}, and try to answer user's query: {state['user_input'][-1]}. Recent chat history: {last_two_chats}\"])\n",
        "    state[\"llm_output\"] = result.text\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def save_photo(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Photo saved\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def take_video(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Video taken\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def error(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Some internal error occurred. Please retry\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"first_user_input\", first_user_input)\n",
        "\n",
        "workflow.add_node(\"take_photo\", take_photo)\n",
        "workflow.add_node(\"fetch_details\", fetch_details)\n",
        "workflow.add_node(\"save_photo\", save_photo)\n",
        "workflow.add_node(\"take_video\", take_video)\n",
        "workflow.add_node(\"error\", error)\n",
        "workflow.add_node(\"web_search\", web_search)\n",
        "workflow.add_node(\"chat_bot\", chat_bot)\n",
        "workflow.set_entry_point(\"first_user_input\")\n",
        "workflow.add_edge(\"take_photo\", END)\n",
        "workflow.add_edge(\"fetch_details\", END)\n",
        "workflow.add_edge(\"save_photo\", END)\n",
        "workflow.add_edge(\"take_video\", END)\n",
        "workflow.add_conditional_edges(\n",
        "    \"first_user_input\",\n",
        "    decision,\n",
        "    {\n",
        "        \"Take a photo\": \"take_photo\",\n",
        "        \"Fetch details from the image\": \"fetch_details\",\n",
        "        \"Save the photo\": \"save_photo\",\n",
        "        \"Take a video\": \"take_video\",\n",
        "        \"Error\": \"error\",\n",
        "        \"Use Chat bot\": \"chat_bot\",\n",
        "        \"Search the web for particular detail\": \"web_search\",\n",
        "    },\n",
        ")\n",
        "\n",
        "gemin_app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "zDJyYo42jTV0",
        "outputId": "a76b0280-0b3e-4210-e2fb-a86d2c552f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the chat system. Type 'exit' to end the conversation.\n",
            "User: eiffel tower\n",
            "Include an image? (yes/no): n\n",
            "{'user_input': ['eiffel tower'], 'photo_description': None, 'feedback': \"Title: Eiffel Tower information : facts, height in feet, weight, ...\\nSnippet: Eiffel tower facts, height & weight ; 377 feet, 4,692 square feet · 906 feet, 820 square feet · 5 lifts from the esplanade to second floor, 2 x 2 duolifts from\\xa0...\\n\\nTitle: 15 Eiffel Tower Facts: History, Science, and Secrets\\nSnippet: Jul 13, 2024 ... The Eiffel Tower was built with the intent of flaunting France's industrial strength during the 1889 World's Fair, and the original plan was to\\xa0...\\n\\nTitle: The Eiffel Tower: all there is to know - Official website\\nSnippet: Little by little, its image was associated with Paris, until it even became its worldwide symbol. Poets, painters, singers, choreographers, film dire... Eiffel\\xa0...\\n\\n\", 'llm_output': \"The Eiffel Tower is a famous landmark in Paris, France. It's known for its height and unique structure. \\n\", 'image_path': None, 'chat_history': ['User: eiffel tower', \"Assistant: The Eiffel Tower is a famous landmark in Paris, France. It's known for its height and unique structure. \\n\"]}\n",
            "Assistant: The Eiffel Tower is a famous landmark in Paris, France. It's known for its height and unique structure. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-3c36894e9eb3>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mrun_chat_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-3c36894e9eb3>\u001b[0m in \u001b[0;36mrun_chat_loop\u001b[0;34m(app)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "def run_chat_loop(app):\n",
        "    state = {\n",
        "        \"user_input\": [],\n",
        "        \"photo_description\": None,\n",
        "        \"feedback\": None,\n",
        "        \"llm_output\": None,\n",
        "        \"image_path\": None,\n",
        "        \"chat_history\": []\n",
        "    }\n",
        "\n",
        "    print(\"Welcome to the chat system. Type 'exit' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_message = input(\"User: \").strip()\n",
        "        if user_message.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Option to include image path\n",
        "        include_image = input(\"Include an image? (yes/no): \").lower().strip() == 'yes'\n",
        "        if include_image:\n",
        "            image_path = input(\"Enter image path: \").strip()\n",
        "            if not image_path:\n",
        "                image_path=\"/content/new-york-city-neighborhoods-flytographer-11.jpeg\"\n",
        "            state[\"image_path\"] = image_path\n",
        "        else:\n",
        "            state[\"image_path\"] = None\n",
        "\n",
        "        # Update user_input and chat_history\n",
        "        state[\"user_input\"] = [user_message]\n",
        "        state[\"chat_history\"].append(f\"User: {user_message}\")\n",
        "\n",
        "        # Keep only the last two exchanges in chat_history\n",
        "        if len(state[\"chat_history\"]) > 4:\n",
        "            state[\"chat_history\"] = state[\"chat_history\"][-4:]\n",
        "\n",
        "        # Invoke the app\n",
        "        result = gemin_app.invoke(state)\n",
        "        print(f\"{result}\")\n",
        "        # Extract the assistant's response\n",
        "        assistant_response = result.get(\"llm_output\", \"No response generated.\")\n",
        "\n",
        "        # Print the assistant's response\n",
        "        print(f\"Assistant: {assistant_response}\")\n",
        "        # Update chat_history with assistant's response\n",
        "        state[\"chat_history\"].append(f\"Assistant: {assistant_response}\")\n",
        "\n",
        "        # Clear previous user input and llm_output\n",
        "        state[\"user_input\"] = []\n",
        "        state[\"llm_output\"] = None\n",
        "\n",
        "    print(\"Chat ended. Thank you for using the system!\")\n",
        "\n",
        "# Usage\n",
        "run_chat_loop(app)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2kPS35Dh4SVK1WT92Sv9vsu1lxt_3nD95SijdDtgqghQTuj1o\n",
        "!ngrok update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK4RkMrzvycr",
        "outputId": "e96c739e-a4bf-4f5b-a58e-71521b99c551"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[08-09|06:13:54] no configuration paths supplied \n",
            "\u001b[36mDBUG\u001b[0m[08-09|06:13:54] ngrok config file at legacy location does not exist \u001b[36mlegacy_path\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[08-09|06:13:54] using configuration at default config path \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[08-09|06:13:54] open config file                         \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml \u001b[32merr\u001b[0m=nil\n",
            "No update available, this is the latest version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOkGuSgdxML9",
        "outputId": "b2ff300c-acbd-4c97-fa22-e02c8601a637"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.5)\n",
            "Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "jNPDOxx9XV6X",
        "outputId": "0d71b31b-8474-4a4b-a3b7-40dfc30f38a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://fa72-34-82-131-61.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Aug/2024 10:53:28] \"OPTIONS /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Aug/2024 10:53:32] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'user_input': ['hi'], 'photo_description': None, 'feedback': None, 'llm_output': \"Hi there!  It's so nice to hear from you. How can I help you today? 😊 \\n\", 'image_path': None, 'chat_history': [\"Assistant: Hi there!  It's so nice to hear from you. How can I help you today? 😊 \\n\"]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Aug/2024 10:54:06] \"OPTIONS /chat HTTP/1.1\" 200 -\n",
            "ERROR:__main__:Exception on /chat [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask_cors/extension.py\", line 178, in wrapped_function\n",
            "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-46-232e2b8c8e13>\", line 34, in chat\n",
            "    state=gemin_app.invoke(state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\", line 1312, in invoke\n",
            "    for chunk in self.stream(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\", line 997, in stream\n",
            "    _panic_or_proceed(done, inflight, loop.step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\", line 1398, in _panic_or_proceed\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langgraph/pregel/executor.py\", line 60, in done\n",
            "    task.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\", line 25, in run_with_retry\n",
            "    task.proc.invoke(task.input, task.config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\", line 2876, in invoke\n",
            "    input = context.run(step.invoke, input, config, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langgraph/utils.py\", line 102, in invoke\n",
            "    ret = context.run(self.func, input, **kwargs)\n",
            "  File \"<ipython-input-28-adab3c25bcb9>\", line 74, in web_search\n",
            "    search_results = google_search(api_key, cx, search_prompt)\n",
            "  File \"<ipython-input-5-0ee7bd62bfb4>\", line 27, in google_search\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyCKfTgJPf-ctHodmIL1s3Ru6yCBIrqdYRE&cx=16f55fa2cee4d4e0b&q=Eiffel+Tower+information+%0A\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Aug/2024 10:54:10] \"\u001b[35m\u001b[1mPOST /chat HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "state = {\n",
        "    \"user_input\": [],\n",
        "    \"photo_description\": None,\n",
        "    \"feedback\": None,\n",
        "    \"llm_output\": None,\n",
        "    \"image_path\": None,\n",
        "    \"chat_history\": []\n",
        "}\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return '''\n",
        "    <html>\n",
        "        <head>\n",
        "            <title>Flask App Test</title>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h1>Welcome to the Flask App</h1>\n",
        "            <p>This is a simple HTML page served by Flask to verify that the app is working correctly.</p>\n",
        "        </body>\n",
        "    </html>\n",
        "    '''\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    global state\n",
        "    data = request.get_json()\n",
        "    state[\"user_input\"] = data.get(\"user_input\", [])\n",
        "    state=gemin_app.invoke(state)\n",
        "    print(state)\n",
        "    response = {\n",
        "        \"user_input\": state[\"user_input\"],\n",
        "        \"photo_description\": state[\"photo_description\"],\n",
        "        \"feedback\": state[\"feedback\"],\n",
        "        \"llm_output\": state[\"llm_output\"],\n",
        "        \"image_path\": state[\"image_path\"],\n",
        "        \"chat_history\": state[\"chat_history\"]\n",
        "    }\n",
        "    return jsonify(response)\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload():\n",
        "    global state\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "    if file:\n",
        "        filename = file.filename\n",
        "        filepath = os.path.join(\"/content\", filename)\n",
        "        file.save(filepath)\n",
        "        state[\"image_path\"] = filepath\n",
        "        return jsonify({\"message\": \"File uploaded successfully\", \"image_path\": filepath}), 200\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    port= 5000\n",
        "    # Terminate any existing tunnels\n",
        "    ngrok.kill()\n",
        "    ngrok.update()\n",
        "    print(ngrok.connect(port).public_url)\n",
        "    app.run(port=port)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "curl --location 'https://c5df-34-82-131-61.ngrok-free.app/chat' \\\n",
        "--header 'Content-Type: application/json' \\\n",
        "--data '{\n",
        "    \"user_input\": [\"Hello, how are you?\"],\n",
        "    \"image_path\": \"/content/sample-image.jpg\"\n",
        "}'"
      ],
      "metadata": {
        "id": "aq0DjodfvbY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqnqWXQWrYF8"
      },
      "outputs": [],
      "source": [
        "query = 'who is Adam driver?'\n",
        "\n",
        "try:\n",
        "    search_results = google_search(api_key, cx, query)\n",
        "    for item in search_results.get('items', []):\n",
        "        print(f\"Title: {item['title']}\")\n",
        "        print(f\"Snippet: {item['snippet']}\")\n",
        "        print(f\"Link: {item['link']}\")\n",
        "        print(\"\\n\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STF5iItEz0kY"
      },
      "outputs": [],
      "source": [
        "cont=True\n",
        "user_input=\"Take that photo\"\n",
        "cache={\"user_input\":None,\"photo_description\":None, \"feedback\": None, \"llm_output\":None,\"image_path\": \"/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg\"}\n",
        "while cont is True:\n",
        "  user_input=input(\"Enter your input: \")\n",
        "  if user_input==\"x\":\n",
        "    cont=False\n",
        "  cache[\"user_input\"]=user_input\n",
        "  cache=app.invoke(cache)\n",
        "  print(cache)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdUexu/qqy9Ae81Zymk59y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}